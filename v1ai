const { OpenAI } = require('openai');
const Tesseract = require('tesseract.js');
const { QdrantClient } = require('qdrant-client');

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const qdrant = new QdrantClient({ url: process.env.QDRANT_URL || 'http://127.0.0.1:6333' });

const COLLECTION = process.env.MEMORY_COLLECTION || 'user_memory';
const EMBED_MODEL = process.env.EMBED_MODEL || 'text-embedding-3-small';

async function ensureCollection() {
  try {
    await qdrant.getCollection(COLLECTION);
  } catch (_) {
    await qdrant.createCollection(COLLECTION, { vectors: { size: 768, distance: 'Cosine' } });
  }
}
ensureCollection().catch(console.error);

async function embed(text) {
  const res = await openai.embeddings.create({ input: text, model: EMBED_MODEL });
  return res.data[0].embedding;
}

async function addToMemory(text) {
  const vector = await embed(text);
  const id = Date.now();
  await qdrant.upsert(COLLECTION, {
    points: [{ id, vector, payload: { text } }]
  });

  // Memory cap
  const { result: existing } = await qdrant.scroll(COLLECTION, { limit: 9999 });
  if (existing.length > 100) {
    const toDelete = existing
      .sort((a, b) => a.id - b.id)
      .slice(0, existing.length - 100)
      .map(p => p.id);
    await qdrant.delete(COLLECTION, { points: toDelete });
  }
}

async function recallMemory(query, k = 3) {
  const vector = await embed(query);
  const { result } = await qdrant.search(COLLECTION, { vector, limit: k });
  return result.map(r => r.payload.text).join('\n');
}

async function processImageAndPrompt(imageBuffer, transcript = '') {
  const { data: { text: ocr } } = await Tesseract.recognize(imageBuffer, 'eng');
  const memory = await recallMemory(ocr.slice(0, 300));

  const b64 = imageBuffer.toString('base64');
  const messages = [
    { role: 'system', content: 'You are a concise AI assistant. Keep answers short.' },
    {
      role: 'user', content: [
        { type: 'image_url', image_url: { url: `data:image/png;base64,${b64}` } },
        { type: 'text', text: `Context OCR: ${ocr.slice(0, 500)}` },
        { type: 'text', text: `Transcript: ${transcript}` },
        { type: 'text', text: `Previous context: ${memory}` },
      ],
    },
  ];
  const { choices } = await openai.chat.completions.create({
    model: process.env.MODEL_GPT,
    messages,
    max_tokens: 180
  });
  return choices[0].message.content.trim();
}

module.exports = { processImageAndPrompt, addToMemory };
